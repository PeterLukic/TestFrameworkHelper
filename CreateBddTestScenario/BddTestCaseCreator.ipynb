{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e30c7c",
   "metadata": {},
   "source": [
    "### ðŸ“„ INSTALLATION PACKAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d9806",
   "metadata": {},
   "source": [
    "#%pip install notebook ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5df92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLM & LangChain Integration ===\n",
    "#%pip install langchain\n",
    "#%pip install langchain-openai\n",
    "#%pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00812f58",
   "metadata": {},
   "source": [
    "# Initialize LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseekcloud_llm = ChatOllama(\n",
    "    model=\"deepseek-v3.1:671b-cloud\",        \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3e064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f587d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic  # or suitable class\n",
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your_api_key_here\"\n",
    "\n",
    "claude_llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4\",  # example model\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812794bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip show langchain\n",
    "#%pip install --upgrade langchain\n",
    "#%pip install -qU langchain-community unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a5a08",
   "metadata": {},
   "source": [
    "### ðŸ“ Generate BDD Test Cases from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38fd02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-document-loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249383f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "def generate_bdd_test_cases_from_pdf(user_story: str) -> str:\n",
    "    \"\"\"\n",
    "    ðŸ“„ Reads requirements from a specific PDF file and generates comprehensive \n",
    "    BDD test scenarios in Gherkin format.\n",
    "    Includes valid, invalid, edge case, and alternative flows.\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_file = \"./Docs/LoginDocumentation.pdf\"\n",
    "    loader = UnstructuredPDFLoader(\n",
    "    file_path=pdf_file,\n",
    "    languages=[\"eng\"]  # explicitly specify English for OCR\n",
    "    )\n",
    "    docs = loader.load()  # returns list of Document objects\n",
    "\n",
    "    # Combine text from all pages (limit to first 2000 chars for safety)\n",
    "    requirements_text = \"\\n\\n\".join([doc.page_content.strip() for doc in docs])[:2000]\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a QA Automation Engineer. \n",
    "        Your task is to convert the following user story into ALL possible test cases \n",
    "        in Gherkin BDD style format.\n",
    "        Include valid, invalid, edge case, and alternative flow scenarios.\n",
    "\n",
    "        {requirements_text}\n",
    "\n",
    "        Format your response as:\n",
    "        Feature: [Feature name]\n",
    "\n",
    "        Scenario: [Scenario name]\n",
    "            Given [precondition]\n",
    "            When [action]\n",
    "            Then [expected result]\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    prompt = prompt_template.format(requirements_text=requirements_text)\n",
    "    response = llama_llm.invoke(prompt)\n",
    "\n",
    "    # Extract content from the response\n",
    "    if hasattr(response, 'content'):\n",
    "        return response.content\n",
    "    else:\n",
    "        return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7bc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "def generate_single_bdd_test_case_from_pdf(user_story: str) -> str:\n",
    "    \"\"\"\n",
    "    ðŸ“„ Reads requirements from a specific PDF file and generates a single \n",
    "    BDD test scenario in Gherkin format.\n",
    "    Includes a combination of valid, invalid, edge case, or alternative flows.\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_file = \"./Docs/LoginDocumentation.pdf\"\n",
    "    loader = UnstructuredPDFLoader(\n",
    "    file_path=pdf_file,\n",
    "    languages=[\"eng\"]  # explicitly specify English for OCR\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Combine text from all pages (limit to first 2000 chars for safety)\n",
    "    requirements_text = \"\\n\\n\".join([doc.page_content.strip() for doc in docs])[:2000]\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are a QA Automation Engineer. \n",
    "        Your task is to convert the following user story into ONLY ONE test case in Gherkin BDD style format.\n",
    "\n",
    "        {requirements_text}\n",
    "\n",
    "        Format your response as:\n",
    "        Feature: [Feature name]\n",
    "\n",
    "        Scenario: [Scenario name]\n",
    "            Given [precondition]\n",
    "            When [action]\n",
    "            Then [expected result]\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    prompt = prompt_template.format(requirements_text=requirements_text)\n",
    "    response = deepseekcloud_llm.invoke(prompt)\n",
    "\n",
    "    # Extract content from the response\n",
    "    if hasattr(response, 'content'):\n",
    "        return response.content\n",
    "    else:\n",
    "        return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27b1e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Generated BDD Test Cases from PDF:\n",
      "\n",
      "Feature: OrangeHRM Login Authentication\n",
      "\n",
      "Scenario: Successful login with valid demo credentials\n",
      "    Given I am on the OrangeHRM login page\n",
      "    When I enter \"Admin\" in the username field and \"admin123\" in the password field\n",
      "    And I click the login button\n",
      "    Then I should be redirected to the OrangeHRM dashboard\n"
     ]
    }
   ],
   "source": [
    "# Make sure your tool is imported\n",
    "# from your_module import generate_bdd_test_cases_from_pdf\n",
    "# and your LLM instance: qwen_llm\n",
    "\n",
    "# Input for the tool\n",
    "user_story_input = \"Generate BDD test cases from PDF\"\n",
    "\n",
    "# Directly call the tool\n",
    "try:\n",
    "    result = generate_single_bdd_test_case_from_pdf(user_story_input)\n",
    "    print(\"ðŸ“„ Generated BDD Test Cases from PDF:\\n\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
